{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlproj_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mdX4pMNkWgCi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdX4pMNkWgCi",
        "colab_type": "text"
      },
      "source": [
        "## **`Mecab` Install on Colab**\n",
        "출처 :  `https://colab.research.google.com/drive/1tL2WjfE0v_es4YJCLGoEJM5NXs_O_ytW#scrollTo=Z7PCBmGrsR4Y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In1eirejcEIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install konlpy       # Python 3.x\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "import os\n",
        "os.chdir('/tmp/')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz\n",
        "!tar zxfv mecab-0.996-ko-0.9.1.tar.gz\n",
        "os.chdir('/tmp/mecab-0.996-ko-0.9.1')\n",
        "!./configure\n",
        "!make\n",
        "!make check\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp/') \n",
        "!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz\n",
        "!tar -zvxf m4-1.4.9.tar.gz\n",
        "os.chdir('/tmp/m4-1.4.9')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -OL http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz\n",
        "!tar xzf autoconf-2.69.tar.gz\n",
        "os.chdir('/tmp/autoconf-2.69')\n",
        "!./configure --prefix=/usr/local\n",
        "!make\n",
        "!make install\n",
        "!export PATH=/usr/local/bin\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -LO http://ftp.gnu.org/gnu/automake/automake-1.11.tar.gz\n",
        "!tar -zxvf automake-1.11.tar.gz\n",
        "os.chdir('/tmp/automake-1.11')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')\n",
        "!ldconfig\n",
        "!ldconfig -p | grep /usr/local/lib\n",
        "!./autogen.sh\n",
        "!./configure\n",
        "!make\n",
        "# !sh -c 'echo \"dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\" > /usr/local/etc/mecabrc'\n",
        "!make install\n",
        "\n",
        "os.chdir('/content')\n",
        "!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
        "os.chdir('/content/mecab-python-0.996')\n",
        "!python3 setup.py build\n",
        "!python3 setup.py install\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "m = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYWQxFXKiBei",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##**Functions for preprocessing**\n",
        "> **`import_normal_dset`** : returns normal news dset _(600000+)_<br>\n",
        "> **`import_bad_dset`** : returns violation-of-law-interest news dset _(5465)_<br>\n",
        "> **`tokenize_title_and_remove_stopword`** : returns tokenized and stopword-removed title list<br>\n",
        "> **`testdata_tag`** : tag 20% of the data to be for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b0SVqoBcUWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
        "import konlpy\n",
        "from konlpy.tag import Mecab\n",
        "import nltk\n",
        "import json\n",
        "import sklearn \n",
        "from pprint import pprint as print\n",
        "import torch\n",
        "import pickle, os, re, glob, ast\n",
        "from matplotlib import font_manager, rc\n",
        "import random\n",
        "\n",
        "category = ['정치', '세계', 'IT과학', '경제', '사회', '생활문화'] #네이버 뉴스 분류와 동일한 카테고리 분류 채택\n",
        "\n",
        "def import_normal_dset():\n",
        "    dlist = []\n",
        "    fname = u'./newsdata/*_raw.csv'\n",
        "    flist = glob.glob(fname)\n",
        "    for f in flist:\n",
        "        if len(dlist) == 0:\n",
        "            dlist = pd.read_csv(f)\n",
        "        else:\n",
        "            dlist = pd.concat([dlist, pd.read_csv(f)], axis = 0)\n",
        "\n",
        "    dlist.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "    dlist.reset_index(inplace = True)\n",
        "    dlist.drop(['index'], axis = 1, inplace = True)\n",
        "    dlist.columns = ['Year','Category', 'Media','Title']\n",
        "    dlist['Reason'] = u'없음'\n",
        "    dlist['Result'] = u'정상'\n",
        "    return dlist\n",
        "\n",
        "def import_bad_dset(state = 'res'):\n",
        "    DataPath = u'./newsdata/'+state+'.csv'\n",
        "    data = pd.read_csv(DataPath)\n",
        "    data.drop(['Unnamed: 0','2'], axis = 'columns', inplace = True)\n",
        "    data.columns = ['Year','Media','Space','Title','Reason','Result']\n",
        "    data['Result'] = data['Result'].fillna('n')\n",
        "    data['Reason'] = data['Reason'].fillna('n')\n",
        "    data = data.dropna()\n",
        "    data = data.reset_index()\n",
        "\n",
        "    for i in range(data.shape[0]-1):  #Raw Data에서 동일한 Reason 항목 연속시 \"로 표기한 것을 원래대로 복구\n",
        "        if len(data['Reason'][i+1]) <=1: data['Reason'][i+1] = data['Reason'][i]\n",
        "        if len(data['Result'][i+1]) <=1: data['Result'][i+1] = data['Result'][i]\n",
        "\n",
        "    df = {}\n",
        "    for i in range(8):\n",
        "        df[2012+i] = data['Year'].str.find(str(i+2012))!=-1\n",
        "\n",
        "    for i in range(8):\n",
        "        data['Year'][df[2012+i]] = 2012+i  #Year formatting\n",
        "\n",
        "    data.drop(['index'], axis = 1, inplace = True)\n",
        "\n",
        "    category = {  # Raw Data에서 Category가 명확하게 분류되지 않음. 다음 단어가 포함된 카테고리를 총 6개의 카테고리로 분류. 나머지는 Others.\n",
        "        '정치':['정치', '여의도', '국회', '시사', '자치'],\n",
        "        '경제':['경제', 'Business', '산업'],\n",
        "        '사회':['사회', '사고', '법원', '지역'],\n",
        "        '생활문화':['문화', '연예', '스타', '라이프', '엔터', '방송', '엔터테인먼트'],\n",
        "        '세계':['국제', '외교', '해외'],\n",
        "        'IT과학':['IT', '과학', '의료']\n",
        "    }\n",
        "\n",
        "    data['Category'] = 'Others'\n",
        "\n",
        "    for cat in category:\n",
        "        for c in category[cat]:\n",
        "            data['Category'][data['Space'].str.find(c)!=-1] = cat\n",
        "            \n",
        "    data['Category'][0:5] = '사회'\n",
        "\n",
        "    for i in range(len(data)):  # 비슷한 카테고리끼리 인접한 데이터배치이므로 주변 카테고리들과 유사한 카테고리로 지정\n",
        "        if data['Category'][i] == 'Others':\n",
        "            data['Category'][i] = data['Category'][i-1:i+2].max() \n",
        "    return data\n",
        "\n",
        "def tokenize_title_and_remove_stopword(dat):\n",
        "    tokenizer = Mecab()\n",
        "    stopwords=['의','가','이','은','들','는','잘','과','도','를','로','으로','자','에','와','한','하다','면','어','다'] # 임의로 지정한 불용어. 기사제목이고 데이터 길이가 짧으므로 최소화한 것.\n",
        "    print(\"Tokenizing\")\n",
        "    tdat = dat['Title'].apply(tokenizer.morphs) # Mecab 사용해 tokenize\n",
        "    print(\"Tokenize Done\")\n",
        "    for i in range(len(tdat)):  # 불용어 제거, Empty data의 경우 [\"0\"]*10으로 padding.\n",
        "        tdat[i] = [word for word in tdat[i] if not word in stopwords and (re.match('[가-힣]+', word))]\n",
        "        if tdat[i] == []:\n",
        "            tdat[i] = ['0']*10\n",
        "    print(\"Remove Stopwords Done\")\n",
        "    return tdat\n",
        "\n",
        "\n",
        "def testdata_tag(all_dset_token): # 20% 데이터에 대해 테스팅용임을 태깅 : bad, normal data 각각에 대하여.\n",
        "    all_dset_token['Test'] = False\n",
        "    all_dset_token['Test'][all_dset_token[all_dset_token['없음'] == 1].sample(frac = 0.2).index] = True\n",
        "    all_dset_token['Test'][all_dset_token[all_dset_token['없음'] != 1].sample(frac = 0.2).index] = True\n",
        "    return all_dset_token\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTalf4fBiHiN",
        "colab_type": "text"
      },
      "source": [
        "##**Load Data**\n",
        "> **`all_dset`** : (611347*5) sampled newstitle dataset from 2012/9/ to 2019/9<br>\n",
        "> **`all_dset_token`** : all_dset title _tokenized_ and _stopword_removed_<br>\n",
        "> **`new_b_dset`** : bad newstitle dataset from 2019/10/ to 2019/11/<br>\n",
        "> **`new_b_dset_token`** : new_b_dset title _tokenized_ and _stopword_removed_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TipcQT-ycYwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "# Raw Data 분포 확인, 각각 세부 침해내역이 priv, social 중 어디에 속하는지 분류\n",
        "priv_key = ['신원공개', '명예훼손', '사생활', '고소고발', '기사제목', '아동']\n",
        "socl_key = ['보도윤리', '차별금지', '범죄묘사', '범죄사건', '범죄수법', '충격', '자살', '폭력', '여론조사', '기사형', '마약', '성관련', '국가적', '재난']\n",
        "\n",
        "\n",
        "# new data importing\n",
        "new_b_dset = import_bad_dset('newres').drop(['Space', 'Media'], axis = 1)\n",
        "new_b_dset['Reason'] = new_b_dset['Reason'].map(lambda x : x.replace(' ', ''))\n",
        "\n",
        "for i in range(len(new_b_dset)):\n",
        "    for key in priv_key:        \n",
        "        if new_b_dset['Reason'][i].find(key) != -1:\n",
        "            new_b_dset['Reason'][i] = 'Private'\n",
        "    for key in socl_key:\n",
        "        if new_b_dset['Reason'][i].find(key) != -1:\n",
        "            new_b_dset['Reason'][i] = 'Social'\n",
        "\n",
        "new_b_dset_token = new_b_dset.copy()\n",
        "new_b_dset_token['Title'] = tokenize_title_and_remove_stopword(new_b_dset)\n",
        "new_b_dset_token = pd.concat([new_b_dset_token.drop(['Reason', 'Result'], axis = 1), pd.get_dummies(new_b_dset['Reason'])], axis = 1)\n",
        "new_b_dset_token['없음']=0\n",
        "new_b_dset_token['Test']=True\n",
        "print('Saving new_b_dset')\n",
        "new_b_dset.to_csv('./newsdata/new_b_dset.csv')\n",
        "\n",
        "\n",
        "# bad data importing\n",
        "b_dset = import_bad_dset().drop(['Space', 'Media'], axis = 1)\n",
        "all_dset = pd.merge(b_dset, import_normal_dset(), how = 'outer').drop(['Media'], axis = 1)\n",
        "all_dset['Reason'] = all_dset['Reason'].map(lambda x : x.replace(' ', ''))\n",
        "\n",
        "for i in range(len(all_dset)):\n",
        "    for key in priv_key:        \n",
        "        if all_dset['Reason'][i].find(key) != -1:\n",
        "            all_dset['Reason'][i] = 'Private'\n",
        "    for key in socl_key:\n",
        "        if all_dset['Reason'][i].find(key) != -1:\n",
        "            all_dset['Reason'][i] = 'Social'\n",
        "\n",
        "all_dset_token = all_dset.copy()\n",
        "all_dset_token['Title'] = tokenize_title_and_remove_stopword(all_dset_token)\n",
        "all_dset_token = pd.concat([all_dset_token.drop(['Reason', 'Result'], axis = 1), pd.get_dummies(all_dset['Reason'])], axis = 1)\n",
        "all_dset_token = testdata_tag(all_dset_token)\n",
        "print('Saving all_dset')\n",
        "all_dset.to_csv('./newsdata/all_dset.csv')\n",
        "print('Saving all_dset_token')\n",
        "all_dset_token.to_csv('./newsdata/all_dset_token.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBX3k6_4ikvW",
        "colab_type": "text"
      },
      "source": [
        "## **Embed**\n",
        "> **`w2v_param`** : Word2Vec parameter set<br>\n",
        "> **`model_construct`** : load if exists, or make Word2Vec model with given all_title_token(train)<br>\n",
        "> **`get_sample`** : sample normal dset and return with bad dset<br>\n",
        "> **`get_tsample`** : return test-tagged dset<br>\n",
        "> **`fitsize`** : make the num of token to be 10 to embed(duplicate or truncate)<br>\n",
        "> **`embed_sample`** : get data from get_sample and embed by constructed model<br>\n",
        "> **`embed_tsample`** : get data from get_tsample and embed by constructed model<br>\n",
        "> **`returnwv`** : return vocab vector if exists, or return dummy vector for OOV vocabs<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVLQpJwiccD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import logging, time, multiprocessing\n",
        "from time import time\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "class w2v_param:\n",
        "    def __init__(self, vs, w, es, e):\n",
        "      self.vector_size = vs\n",
        "      self.window = w\n",
        "      self.epochs = es\n",
        "      self.epoch = e\n",
        "      self.min_count = 1\n",
        "\n",
        "w2v_param = w2v_param(32, 3, 10, 4) # Embedding metadata 객체\n",
        "\n",
        "def model_construct(param_dic = w2v_param, all_title_token = all_dset_token['Title'], mode = 'word', load = True):\n",
        "    model_name = './modeldata/Word2Vec('+mode+'_dset, E'+str(param_dic.epoch)+'-vs'+str(param_dic.vector_size)+'w'+str(param_dic.window)+'e'+str(param_dic.epochs)+'mc'+str(param_dic.min_count)+')'\n",
        "    if load and os.path.exists(model_name):\n",
        "        model = Word2Vec.load(model_name)\n",
        "    else:\n",
        "        model = Word2Vec(size = param_dic.vector_size, window = param_dic.window, iter = param_dic.epochs, min_count = param_dic.min_count, workers = cores)    # Learning\n",
        "        model.build_vocab(all_title_token)\n",
        "        start = time()\n",
        "        for epoch in range(param_dic.epoch):  # alpha decaying하며 반복학습\n",
        "            model.train(all_title_token, total_examples = model.corpus_count, epochs = model.iter)\n",
        "            model.alpha-=0.002\n",
        "            model.min_alpha = model.alpha\n",
        "        end = time()\n",
        "        print(\"Time : {}s\".format(end-start))\n",
        "        model.save(model_name)\n",
        "    return model\n",
        "\n",
        "def get_sample(all_dset_token = all_dset_token):\n",
        "    b_dset = all_dset_token[all_dset_token['없음'] != 1]\n",
        "    b_dset = b_dset[b_dset['Test'] == False]  #non-testing data 얻음\n",
        "    n_dset = all_dset_token[all_dset_token['없음'] == 1]\n",
        "    n_dset = n_dset[n_dset['Test'] == False]  #non-testing data 얻음\n",
        "    sample_rate = pd.crosstab(b_dset.Year, b_dset.Category)\n",
        "    total_sample_num = b_dset.shape[0]//2 # balanced sampling : bad 2 : normal 1의 비율로 normal data sampling\n",
        "    sample_num = sample_rate/sample_rate.sum().sum() * total_sample_num\n",
        "    sample_num = sample_num.round(0).astype(int)\n",
        "    sample = []\n",
        "    for cat in sample_num.columns:\n",
        "        for year in sample_num.index:\n",
        "            smp = n_dset[:][(n_dset['Year'] == year) & (n_dset['Category'] == cat)]\n",
        "            if len(smp) == 0: smp = n_dset\n",
        "            if sample_num[cat][year] != 0:\n",
        "                smp = smp.sample(n = sample_num[cat][year])\n",
        "                if len(sample) == 0: sample = smp\n",
        "                else: sample = pd.concat([sample, smp], axis = 0)\n",
        "\n",
        "        sample = sample.reset_index().drop(['index'], axis = 1)\n",
        "    return pd.concat([sample, b_dset], axis = 0).reset_index().drop(['index', 'Category', 'Year', 'Test'], axis = 1)\n",
        "\n",
        "def get_tsample(all_dset_token = all_dset_token):\n",
        "    return all_dset_token[all_dset_token['Test'] == True].reset_index().drop(['index', 'Category', 'Year', 'Test'], axis = 1)\n",
        "\n",
        "def fitsize(target, targetlen):\n",
        "    processed_dset = []\n",
        "    for token in target:\n",
        "        curlen = len(token)\n",
        "        if curlen == 0: # empty data를 padding으로 채움\n",
        "            processed_dset.append(['0']*10)\n",
        "        else: # 길이가 부족한 데이터의 경우 반복을 통해 채움\n",
        "            while len(token) < targetlen:\n",
        "                token += token\n",
        "            processed_dset.append(token[:targetlen])  # targetlen만큼 truncate\n",
        "    \n",
        "    return processed_dset\n",
        "\n",
        "def embed_sample(param_dic = w2v_param, token = all_dset_token, mode = 'word'):\n",
        "    sample = get_sample(token)\n",
        "    sample.Title = fitsize(sample.Title, 10)\n",
        "    model = model_construct(param_dic, token.Title)\n",
        "    train_data = torch.tensor(sample.Title.apply(lambda x : list(map(lambda y : model.wv[y], x))))  # 주어진 토큰의 임베딩 벡터를 얻음\n",
        "    train_label = torch.tensor(np.array(sample.drop('Title', axis = 1)))\n",
        "    return train_data, train_label\n",
        "\n",
        "def returnwv(model, y): # out-of-vocab 단어의 경우 padding의 단어벡터를 사용\n",
        "    res = ''\n",
        "    try:\n",
        "        res = model.wv[y]\n",
        "    except:\n",
        "        res = model.wv['0']\n",
        "    return res\n",
        "\n",
        "def embed_tsample(param_dic = w2v_param, token = all_dset_token, mode = 'word'):\n",
        "    sample = get_tsample(token)\n",
        "    sample.Title = fitsize(sample.Title, 10)\n",
        "    model = model_construct(param_dic, token.Title)\n",
        "    test_data = torch.tensor(sample.Title.apply(lambda x : list(map(lambda y : returnwv(model, y), x))))\n",
        "    test_label = torch.tensor(np.array(sample.drop('Title', axis = 1)))\n",
        "    return test_data, test_label\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdtO9uN8irJ8",
        "colab_type": "text"
      },
      "source": [
        "##**Training `dset_vector` by FNN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY4evX8kdink",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "BATCH_SIZE = 240\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        drop1 = nn.Dropout(0.5)\n",
        "        linear1 = nn.Linear(320, 256)\n",
        "        linear3 = nn.Linear(256, 3)\n",
        "        nn.init.xavier_uniform_(linear1.weight)\n",
        "        nn.init.xavier_uniform_(linear3.weight)\n",
        "        self.fnn_module = nn.Sequential(\n",
        "            linear1,\n",
        "            nn.LeakyReLU(),\n",
        "            drop1,\n",
        "            linear3,\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            self.fnn_module = self.fnn_module.cuda()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fnn_module(x).reshape(-1,3)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data_vector, label_vector):\n",
        "        shape = data_vector.shape\n",
        "        self.len = shape[0]\n",
        "        self.tr_X = data_vector.reshape(shape[0], shape[1]*shape[2])\n",
        "        self.tr_Y = label_vector\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tr_X[index], self.tr_Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "test_vector, test_label = embed_tsample()\n",
        "test_dataset = NewsDataset(test_vector, test_label)\n",
        "\n",
        "LEARNING_RATE, EP = 2e-3, 10\n",
        "\n",
        "try:\n",
        "    del model, tr_X, tr_Y, ts_X, ts_Y\n",
        "except:\n",
        "    pass\n",
        "\n",
        "total_loss = []\n",
        "test_loss = []\n",
        "model = Model()\n",
        "if torch.cuda.is_available():\n",
        "    torch.device('cuda')\n",
        "    model = model.cuda()\n",
        "\n",
        "for i in range(10):\n",
        "    train_vector, train_label = embed_sample()  # 매 반복마다 sampling을 다시 함(embed_sample은 새로운 get_sample을 수반)\n",
        "    train_dataset = NewsDataset(train_vector, train_label)\n",
        "    train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = cores, drop_last = False)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EP):\n",
        "        train_loss = 0.0\n",
        "                \n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Train\n",
        "            tr_X, tr_Y = data\n",
        "            tr_X, tr_Y = Variable(tr_X), Variable(tr_Y).long()\n",
        "            if torch.cuda.is_available():\n",
        "                tr_X = tr_X.cuda()\n",
        "                tr_Y = tr_Y.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(tr_X)\n",
        "            loss = crit(y_pred, torch.max(tr_Y,1)[1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            total_loss.append(loss.item())\n",
        "            # Test\n",
        "            model.eval()\n",
        "            ts_X, ts_Y = Variable(test_dataset.tr_X).cuda(), Variable(test_dataset.tr_Y).long().cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(ts_X)\n",
        "            test_loss_val = crit(y_pred, torch.max(ts_Y, 1)[1]).item()\n",
        "            test_loss.append(test_loss_val)\n",
        "            model.train()\n",
        "            del loss, y_pred\n",
        "        print(\"LR = {:.4f} : epoch = {:3d} : loss = {:.4f} : testloss = {:.4f}\".format(LEARNING_RATE, epoch, train_loss/(i+1), test_loss_val))\n",
        "        train_loss = 0.0\n",
        "\n",
        "    plt.plot(total_loss)\n",
        "    plt.plot(test_loss, color = 'RED')\n",
        "    plt.show()\n",
        "\n",
        "model_name = './modeldata/'+'FNN_E'+str(epoch)+\"320_256_3\"+\"_B\"+str(BATCH_SIZE)+\"_\"+str(i)\n",
        "lossdat = pd.DataFrame({'train':total_loss, 'test':test_loss})\n",
        "lossdat.to_csv(model_name+\"_loss.csv\")\n",
        "torch.save(model.state_dict(), model_name)\n",
        "\n",
        "# evaluation for training, test, new data\n",
        "model.eval()\n",
        "ans = torch.max(test_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(test_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Test Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "ans = torch.max(train_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(train_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Train Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "new_bad_vector, new_bad_label = embed_tsample(token = new_b_dset_token)\n",
        "new_bad_dataset = NewsDataset(new_bad_vector, new_bad_label)\n",
        "ans = torch.max(new_bad_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(new_bad_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Newdat Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZgA3pOi8se",
        "colab_type": "text"
      },
      "source": [
        "##**Training `dset_vector` by CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4elIsNy-i9Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "BATCH_SIZE = 240\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        conv1 = nn.Conv2d(1, 8, 4, stride = 1, padding_mode='zeros')\n",
        "        conv2 = nn.Conv2d(1, 8, 5, stride = 1, padding_mode='zeros')\n",
        "        conv_f1 = nn.Conv2d(8, 5, 1, stride = 1)\n",
        "        conv_f2 = nn.Conv2d(8, 4, 1, stride = 1)\n",
        "        conv1r = nn.Conv2d(5, 8, 3, stride = 1, padding_mode='zeros')\n",
        "        conv2r = nn.Conv2d(4, 8, 4, stride = 1, padding_mode='zeros')\n",
        "        conv_f1f = nn.Conv2d(8, 4, 1, stride = 1)\n",
        "        conv_f2f = nn.Conv2d(8, 3, 1, stride = 1)\n",
        "        \n",
        "        linear = nn.Linear(369, 3)\n",
        "        nn.init.xavier_uniform_(conv1.weight)\n",
        "        nn.init.xavier_uniform_(conv2.weight)\n",
        "        nn.init.xavier_uniform_(conv_f1.weight)\n",
        "        nn.init.xavier_uniform_(conv_f2.weight)\n",
        "        nn.init.xavier_uniform_(conv1r.weight)\n",
        "        nn.init.xavier_uniform_(conv2r.weight)\n",
        "        nn.init.xavier_uniform_(conv_f1f.weight)\n",
        "        nn.init.xavier_uniform_(conv_f2f.weight)\n",
        "        nn.init.xavier_uniform_(linear.weight)\n",
        "        self.cnn1_module = nn.Sequential(\n",
        "            conv1,\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 1),\n",
        "            conv_f1,\n",
        "            conv1r,\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 1),\n",
        "            conv_f1f,\n",
        "        )\n",
        "        self.cnn2_module = nn.Sequential(\n",
        "            conv2,\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 1),\n",
        "            conv_f2,\n",
        "            conv2r,\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(2, 1),\n",
        "            conv_f2f,\n",
        "        )\n",
        "        self.fnn_module = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            linear,\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cnn1_module = self.cnn1_module.cuda()\n",
        "            self.cnn2_module = self.cnn2_module.cuda()\n",
        "            self.fnn_module = self.fnn_module.cuda()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        len = x.shape[0]\n",
        "        x = x.reshape(len, 1, x.shape[1], x.shape[2])\n",
        "        cnn1 = self.cnn1_module(x).reshape(len, -1)\n",
        "        cnn2 = self.cnn2_module(x).reshape(len, -1)\n",
        "        cnn = torch.cat((cnn1, cnn2), dim=1)\n",
        "        res = self.fnn_module(cnn)\n",
        "        return res\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data_vector, label_vector):\n",
        "        shape = data_vector.shape\n",
        "        self.len = shape[0]\n",
        "        self.tr_X = data_vector\n",
        "        self.tr_Y = label_vector\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tr_X[index], self.tr_Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "test_vector, test_label = embed_tsample()\n",
        "test_dataset = NewsDataset(test_vector, test_label)\n",
        "\n",
        "LEARNING_RATE, EP = 2e-3, 10\n",
        "\n",
        "try:\n",
        "    del model, tr_X, tr_Y, ts_X, ts_Y\n",
        "except:\n",
        "    pass\n",
        "\n",
        "total_loss = []\n",
        "test_loss = []\n",
        "model = Model()\n",
        "if torch.cuda.is_available():\n",
        "    torch.device('cuda')\n",
        "    model = model.cuda()\n",
        "\n",
        "for i in range(10):\n",
        "    train_vector, train_label = embed_sample()  # 매 반복마다 sampling을 다시 함(embed_sample은 새로운 get_sample을 수반)\n",
        "    train_dataset = NewsDataset(train_vector, train_label)\n",
        "    train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = cores, drop_last = False)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EP):\n",
        "        train_loss = 0.0\n",
        "                \n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Train\n",
        "            tr_X, tr_Y = data\n",
        "            tr_X, tr_Y = Variable(tr_X), Variable(tr_Y).long()\n",
        "            if torch.cuda.is_available():\n",
        "                tr_X = tr_X.cuda()\n",
        "                tr_Y = tr_Y.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(tr_X)\n",
        "            loss = crit(y_pred, torch.max(tr_Y,1)[1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            total_loss.append(loss.item())\n",
        "            # Test\n",
        "            model.eval()\n",
        "            ts_X, ts_Y = Variable(test_dataset.tr_X).cuda(), Variable(test_dataset.tr_Y).long().cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(ts_X)\n",
        "            test_loss_val = crit(y_pred, torch.max(ts_Y, 1)[1]).item()\n",
        "            test_loss.append(test_loss_val)\n",
        "            model.train()\n",
        "            del loss, y_pred\n",
        "        print(\"LR = {:.4f} : epoch = {:3d} : loss = {:.4f} : testloss = {:.4f}\".format(LEARNING_RATE, epoch, train_loss/(i+1), test_loss_val))\n",
        "        train_loss = 0.0\n",
        "\n",
        "    plt.plot(total_loss)\n",
        "    plt.plot(test_loss, color = 'RED')\n",
        "    plt.show()\n",
        "model_name = './modeldata/'+'CNN_E'+str(epoch)+\"50_4*8*5*8*4_5*8*4*8*3\"+\"_B\"+str(BATCH_SIZE)+\"_\"+str(i)\n",
        "lossdat = pd.DataFrame({'train':total_loss, 'test':test_loss})\n",
        "lossdat.to_csv(model_name+\"_loss.csv\")\n",
        "torch.save(model.state_dict(), model_name)\n",
        "\n",
        "# evaluation for training, test, new data\n",
        "model.eval()\n",
        "ans = torch.max(test_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(test_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Test Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "ans = torch.max(train_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(train_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Train Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "new_bad_vector, new_bad_label = embed_tsample(token = new_b_dset_token)\n",
        "new_bad_dataset = NewsDataset(new_bad_vector, new_bad_label)\n",
        "ans = torch.max(new_bad_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(new_bad_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Newdat Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIPnERn1jHZx",
        "colab_type": "text"
      },
      "source": [
        "## **Training `doc_vector` by `GRU`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p4sFiQpjHv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "BATCH_SIZE = 240\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        gru = nn.GRU(input_size = 32, hidden_size = 24, num_layers = 3, dropout = 0.5, bidirectional = True, batch_first = True)\n",
        "        linear = nn.Linear(480, 3)\n",
        "        nn.init.xavier_uniform_(linear.weight)\n",
        "        self.gru_module = nn.Sequential(\n",
        "            gru,\n",
        "        )\n",
        "        self.linear_module = nn.Sequential(\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            linear,\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            self.gru_module = self.gru_module.cuda()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        g = self.gru_module(x)\n",
        "        return self.linear_module(g[0].reshape(g[0].shape[0], -1))\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data_vector, label_vector):\n",
        "        shape = data_vector.shape\n",
        "        self.len = shape[0]\n",
        "        self.tr_X = data_vector\n",
        "        self.tr_Y = label_vector\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tr_X[index], self.tr_Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "test_vector, test_label = embed_tsample()\n",
        "test_dataset = NewsDataset(test_vector, test_label)\n",
        "\n",
        "LEARNING_RATE, EP = 5e-3, 10\n",
        "\n",
        "try:\n",
        "    del model, tr_X, tr_Y, ts_X, ts_Y\n",
        "except:\n",
        "\n",
        "    pass\n",
        "\n",
        "total_loss = []\n",
        "test_loss = []\n",
        "model = Model()\n",
        "if torch.cuda.is_available():\n",
        "    torch.device('cuda')\n",
        "    model = model.cuda()\n",
        "\n",
        "for i in range(10):\n",
        "    train_vector, train_label = embed_sample()  # 매 반복마다 sampling을 다시 함(embed_sample은 새로운 get_sample을 수반)\n",
        "    train_dataset = NewsDataset(train_vector, train_label)\n",
        "    train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = cores, drop_last = False)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EP):\n",
        "        train_loss = 0.0\n",
        "                \n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Train\n",
        "            tr_X, tr_Y = data\n",
        "            tr_X, tr_Y = Variable(tr_X), Variable(tr_Y).long()\n",
        "            if torch.cuda.is_available():\n",
        "                tr_X = tr_X.cuda()\n",
        "                tr_Y = tr_Y.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(tr_X)\n",
        "            loss = crit(y_pred, torch.max(tr_Y,1)[1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            total_loss.append(loss.item())\n",
        "            # Test\n",
        "            model.eval()\n",
        "            ts_X, ts_Y = Variable(test_dataset.tr_X).cuda(), Variable(test_dataset.tr_Y).long().cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(ts_X)\n",
        "            test_loss_val = crit(y_pred, torch.max(ts_Y, 1)[1]).item()\n",
        "            test_loss.append(test_loss_val)\n",
        "            model.train()\n",
        "            del loss, y_pred\n",
        "        print(\"LR = {:.4f} : epoch = {:3d} : loss = {:.4f} : testloss = {:.4f}\".format(LEARNING_RATE, epoch, train_loss/(i+1), test_loss_val))\n",
        "        train_loss = 0.0\n",
        "\n",
        "    plt.plot(total_loss)\n",
        "    plt.plot(test_loss, color = 'RED')\n",
        "    plt.show()\n",
        "model_name = './modeldata/'+'GRU_E'+str(epoch)+\"240_3\"+\"_B\"+str(BATCH_SIZE)+\"_\"+str(i)\n",
        "lossdat = pd.DataFrame({'train':total_loss, 'test':test_loss})\n",
        "lossdat.to_csv(model_name+\"_loss.csv\")\n",
        "torch.save(model.state_dict(), model_name)\n",
        "\n",
        "# evaluation for training, test, new data\n",
        "model.eval()\n",
        "ans = torch.max(test_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(test_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Test Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "ans = torch.max(train_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(train_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Train Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "new_bad_vector, new_bad_label = embed_tsample(token = new_b_dset_token)\n",
        "new_bad_dataset = NewsDataset(new_bad_vector, new_bad_label)\n",
        "ans = torch.max(new_bad_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(new_bad_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Newdat Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJtEdxySjL6k",
        "colab_type": "text"
      },
      "source": [
        "## **Training `doc_vector` by GRU and CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fUQBjaNjUmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "BATCH_SIZE = 240\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        gru = nn.GRU(input_size = 32, hidden_size = 18, num_layers = 2, dropout = 0.5, bidirectional = True, batch_first = True)\n",
        "        conv1 = nn.Conv2d(1, 8, 4, stride = 1, padding_mode='zeros')\n",
        "        conv2 = nn.Conv2d(8, 3, 4, stride = 1, padding_mode='zeros')\n",
        "        linear = nn.Linear(360, 3)\n",
        "        nn.init.xavier_uniform_(conv1.weight)\n",
        "        nn.init.xavier_uniform_(conv2.weight)\n",
        "        nn.init.xavier_uniform_(linear.weight)\n",
        "        self.gru_module = nn.Sequential(\n",
        "            gru,\n",
        "        )\n",
        "        self.cnn_module = nn.Sequential(\n",
        "            nn.LeakyReLU(),\n",
        "            conv1,\n",
        "            nn.LeakyReLU(),\n",
        "            conv2,\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.linear_module = nn.Sequential(\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            linear,\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            self.gru_module = self.gru_module.cuda()\n",
        "            self.cnn_module = self.cnn_module.cuda()\n",
        "            self.linear_module = self.linear_module.cuda()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        gru = self.gru_module(x)\n",
        "        len = gru[0].shape[0]\n",
        "        gru = gru[0].reshape(len, 1, gru[0].shape[1], gru[0].shape[2])\n",
        "        cnn = self.cnn_module(gru).reshape(len, -1)\n",
        "        lin = self.linear_module(cnn)\n",
        "        return lin\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data_vector, label_vector):\n",
        "        shape = data_vector.shape\n",
        "        self.len = shape[0]\n",
        "        self.tr_X = data_vector\n",
        "        self.tr_Y = label_vector\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tr_X[index], self.tr_Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "test_vector, test_label = embed_tsample()\n",
        "test_dataset = NewsDataset(test_vector, test_label)\n",
        "\n",
        "LEARNING_RATE, EP = 5e-3, 10\n",
        "\n",
        "try:\n",
        "    del model, tr_X, tr_Y, ts_X, ts_Y\n",
        "except:\n",
        "\n",
        "    pass\n",
        "\n",
        "total_loss = []\n",
        "test_loss = []\n",
        "model = Model()\n",
        "if torch.cuda.is_available():\n",
        "    torch.device('cuda')\n",
        "    model = model.cuda()\n",
        "\n",
        "for i in range(10):\n",
        "    train_vector, train_label = embed_sample()  # 매 반복마다 sampling을 다시 함(embed_sample은 새로운 get_sample을 수반)\n",
        "    train_dataset = NewsDataset(train_vector, train_label)  \n",
        "    train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = cores, drop_last = False)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(EP):\n",
        "        train_loss = 0.0\n",
        "                \n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Train\n",
        "            tr_X, tr_Y = data\n",
        "            tr_X, tr_Y = Variable(tr_X), Variable(tr_Y).long()\n",
        "            if torch.cuda.is_available():\n",
        "                tr_X = tr_X.cuda()\n",
        "                tr_Y = tr_Y.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(tr_X)\n",
        "            loss = crit(y_pred, torch.max(tr_Y,1)[1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            total_loss.append(loss.item())\n",
        "            # Test\n",
        "            model.eval()\n",
        "            ts_X, ts_Y = Variable(test_dataset.tr_X).cuda(), Variable(test_dataset.tr_Y).long().cuda()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(ts_X)\n",
        "            test_loss_val = crit(y_pred, torch.max(ts_Y, 1)[1]).item()\n",
        "            test_loss.append(test_loss_val)\n",
        "            model.train()\n",
        "            del loss, y_pred\n",
        "        print(\"LR = {:.4f} : epoch = {:3d} : loss = {:.4f} : testloss = {:.4f}\".format(LEARNING_RATE, epoch, train_loss/(i+1), test_loss_val))\n",
        "        train_loss = 0.0\n",
        "\n",
        "    plt.plot(total_loss)\n",
        "    plt.plot(test_loss, color = 'RED')\n",
        "    plt.show()\n",
        "model_name = './modeldata/'+'GRU_CNNE'+str(epoch)+\"240_3\"+\"_B\"+str(BATCH_SIZE)+\"_\"+str(i)\n",
        "lossdat = pd.DataFrame({'train':total_loss, 'test':test_loss})\n",
        "lossdat.to_csv(model_name+\"_loss.csv\")\n",
        "torch.save(model.state_dict(), model_name)\n",
        "\n",
        "# evaluation for training, test, new data\n",
        "model.eval()\n",
        "ans = torch.max(test_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(test_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Test Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "ans = torch.max(train_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(train_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Train Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))\n",
        "new_bad_vector, new_bad_label = embed_tsample(token = new_b_dset_token)\n",
        "new_bad_dataset = NewsDataset(new_bad_vector, new_bad_label)\n",
        "ans = torch.max(new_bad_label, 1)[1].to(\"cuda\")\n",
        "pred = torch.max(model(new_bad_dataset.tr_X.cuda()), 1)[1]\n",
        "print(\"Newdat Accuracy : {}\".format((pred == ans).sum()/(pred.shape[0]*1.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}